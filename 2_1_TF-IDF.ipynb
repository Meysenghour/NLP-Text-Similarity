{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithm : TF-IDF with Consine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import sys\n",
    "\n",
    "# Load the cleaned data\n",
    "data = pd.read_csv('clean_article_210324.csv')\n",
    "\n",
    "# Load Spacy model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Tokenize, lemmatize, and remove stop words\n",
    "# def preprocess_text(text):\n",
    "#     doc = nlp(text)\n",
    "#     tokens = [token.lemma_ for token in doc if not token.is_stop]\n",
    "#     return ' '.join(tokens)\n",
    "def preprocess_text(text):\n",
    "    if isinstance(text, str):\n",
    "        doc = nlp(text)\n",
    "        tokens = [token.lemma_ for token in doc if not token.is_stop]\n",
    "        processed_text = ' '.join(tokens)\n",
    "        return processed_text\n",
    "    else:\n",
    "        return \"\"\n",
    "# Apply preprocessing to introduction, title, and body columns\n",
    "data['intro_processed'] = data['intro'].apply(preprocess_text)\n",
    "data['title_processed'] = data['title'].apply(preprocess_text)\n",
    "data['body_processed'] = data['body'].apply(preprocess_text)\n",
    "\n",
    "# Combine processed text columns\n",
    "data['combined_processed'] = data['intro_processed'] + ' ' + data['title_processed'] + ' ' + data['body_processed']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "# Vectorize text using TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(data['combined_processed'])\n",
    "X = vectorizer.transform(data['combined_processed'])\n",
    "# X = vectorizer.fit_transform(data['combined_processed'])\n",
    "# Calculate cosine similarity matrix\n",
    "cosine_sim = cosine_similarity(X, X)\n",
    "\n",
    "def recommend_articles(input_text, num_recommendations=5):\n",
    "    # Preprocess the input text\n",
    "    input_processed = preprocess_text(input_text)\n",
    "    \n",
    "    # Vectorize the input text\n",
    "    input_vectorized = vectorizer.transform([input_processed])\n",
    "    \n",
    "    # Calculate cosine similarity with all articles\n",
    "    similarity_scores = cosine_similarity(input_vectorized, X)\n",
    "    \n",
    "    # Sort the indices based on similarity scores\n",
    "    sim_indices = similarity_scores.argsort()[0][::-1]\n",
    "    \n",
    "    # Get the recommended articles based on similarity, excluding the input article\n",
    "    recommended_articles = data.iloc[sim_indices[1:num_recommendations+1]].copy()  \n",
    "    \n",
    "    # Add similarity scores to recommended_articles\n",
    "    recommended_articles.loc[:, 'similarity_scores'] = similarity_scores[0][sim_indices[1:num_recommendations+1]]  \n",
    "    \n",
    "    # Set display option for unlimited column width\n",
    "    pd.set_option('display.max_colwidth', None)\n",
    "    \n",
    "    # Return the recommended articles\n",
    "    return recommended_articles[['page_ptr_id','title','published_at', 'similarity_scores', 'intro']]\n",
    "\n",
    "# Example usage\n",
    "input_title = \"UK University Students to Empower Cambodian Girls with STEM Skills\" \n",
    "recommended_articles = recommend_articles(input_title)\n",
    "print(\"Recommended Articles:\")\n",
    "print(recommended_articles)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithm : TF-IDF with Jaccard similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define Jaccard similarity function\n",
    "def jaccard_similarity(set1, set2):\n",
    "    intersection = len(set1.intersection(set2))\n",
    "    union = len(set1.union(set2))\n",
    "    return intersection / union if union != 0 else 0\n",
    "\n",
    "# Calculate Jaccard similarity matrix\n",
    "num_docs = len(data)\n",
    "jaccard_sim = [[0] * num_docs for _ in range(num_docs)]  # Initialize similarity matrix\n",
    "for i in range(num_docs):\n",
    "    for j in range(i+1, num_docs):\n",
    "        similarity = jaccard_similarity(set(data['combined_processed'][i].split()), set(data['combined_processed'][j].split()))\n",
    "        jaccard_sim[i][j] = similarity\n",
    "        jaccard_sim[j][i] = similarity  # Since the matrix is symmetric\n",
    "\n",
    "# Example usage\n",
    "doc_index = 0  # Index of the document for which you want similar documents\n",
    "similar_docs = [(i, jaccard_sim[doc_index][i]) for i in range(num_docs)]  # Similarity scores for all documents\n",
    "similar_docs.sort(key=lambda x: x[1], reverse=True)  # Sort by similarity score\n",
    "print(\"Top 5 similar documents:\")\n",
    "for doc_index, similarity_score in similar_docs[:5]:\n",
    "    print(f\"Document {doc_index}: Similarity Score = {similarity_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Jaccard similarity function\n",
    "def jaccard_similarity(set1, set2):\n",
    "    intersection = len(set1.intersection(set2))\n",
    "    union = len(set1.union(set2))\n",
    "    return intersection / union if union != 0 else 0\n",
    "def recommend_articles(input_text, num_recommendations=6):\n",
    "    # Find the row where the title matches the input_text\n",
    "    row = data[data['title'] == input_text]\n",
    "\n",
    "    # Check if there is a matching row in the dataset\n",
    "    if len(row) > 0:\n",
    "        # Concatenate the title, intro, and body of the row\n",
    "        input_processed = preprocess_text(row['title'].values[0] + ' ' + row['intro'].values[0] + ' ' + row['body'].values[0])\n",
    "    else:\n",
    "        # Preprocess the input text directly\n",
    "        input_processed = preprocess_text(input_text)\n",
    "    \n",
    "    # Calculate Jaccard similarity with all articles\n",
    "    similarity_scores = [jaccard_similarity(set(input_processed.split()), set(article.split())) for article in data['combined_processed']]\n",
    "    \n",
    "    # Sort the indices based on similarity scores\n",
    "    sim_indices = sorted(range(len(similarity_scores)), key=lambda i: similarity_scores[i], reverse=True)[:num_recommendations]\n",
    "    \n",
    "    # Get the recommended articles based on similarity\n",
    "    recommended_articles = data.iloc[sim_indices].copy()  # Make a copy to avoid modifying the original DataFrame\n",
    "    \n",
    "    # Add similarity scores to recommended_articles\n",
    "    recommended_articles.loc[:, 'similarity_scores'] = [similarity_scores[i] for i in sim_indices]  # Using .loc to set values\n",
    "    \n",
    "    # Set display option for unlimited column width\n",
    "    pd.set_option('display.max_colwidth', None)\n",
    "    \n",
    "    # Return the recommended articles\n",
    "    return recommended_articles[['page_ptr_id','title','published_at', 'similarity_scores', 'intro']]\n",
    "\n",
    "# Example usage\n",
    "# Example usage\n",
    "title_1 = \"Annual HEINEKEN Sustainathon Invites University Students to Innovate for a Circular Cambodia\"\n",
    "title_2 = \"For Social Platforms, the Outage Was Short. But People’s Stories Vanished, and That’s No Small Thing\"\n",
    "title_3 = \"Taylor Swift Struck a Deal With Singapore Not to Perform in Any Other Southeast Asian Country\"\n",
    "title_4 = \"UK University Students to Empower Cambodian Girls with STEM Skills\"\n",
    "title_5 = \"Court Detains Lamborghini Driver Behind Fatal Collision\"\n",
    "title_6 = \"AirAsia Cambodia: Enhance Intra-ASEAN Travel, Cambodian Income With Relaxed Visa Fees\"\n",
    "input_title = title_6\n",
    "recommended_articles = recommend_articles(input_title)\n",
    "print(\"Recommended Articles:\")\n",
    "recommended_articles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithm : TF-IDF with Euclidean_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "# Define TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the combined processed text data\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(data['combined_processed'])\n",
    "\n",
    "# Adjust the recommend_articles function to use Euclidean distance\n",
    "def recommend_articles(input_text, num_recommendations=6):\n",
    "    # Preprocess the input text\n",
    "    input_processed = preprocess_text(input_text)\n",
    "    \n",
    "    # Transform the input text using the TF-IDF vectorizer\n",
    "    input_vectorized = tfidf_vectorizer.transform([input_processed])\n",
    "    \n",
    "    # # Calculate Euclidean distance with all articles\n",
    "    # distances = [np.linalg.norm(input_vectorized - article) for article in tfidf_matrix]\n",
    "    # Calculate Euclidean distance with all articles\n",
    "    distances = [euclidean_distances(input_vectorized, doc) for doc in tfidf_matrix]\n",
    "\n",
    "    # Compute similarity scores based on distances (subtract from 1)\n",
    "    similarity_scores = [1 - distance for distance in distances]\n",
    "    \n",
    "    # Sort the indices based on distance\n",
    "    sorted_indices = sorted(range(len(distances)), key=lambda i: distances[i])\n",
    "    \n",
    "    # Get the recommended articles based on distance\n",
    "    recommended_articles = data.iloc[sorted_indices[:num_recommendations]].copy()  # Make a copy to avoid modifying the original DataFrame\n",
    "    \n",
    "    # Add distances to recommended_articles\n",
    "    recommended_articles.loc[:, 'distances'] = [distances[i] for i in sorted_indices[:num_recommendations]]  # Using .loc to set values\n",
    "    \n",
    "    # Set display option for unlimited column width\n",
    "    pd.set_option('display.max_colwidth', None)\n",
    "    \n",
    "    # Return the recommended articles\n",
    "    return recommended_articles[['page_ptr_id','title','published_at', 'distances', 'intro']]\n",
    "\n",
    "# Example usage\n",
    "title_1 = \"Annual HEINEKEN Sustainathon Invites University Students to Innovate for a Circular Cambodia\"\n",
    "title_2 = \"For Social Platforms, the Outage Was Short. But People’s Stories Vanished, and That’s No Small Thing\"\n",
    "title_3 = \"Taylor Swift Struck a Deal With Singapore Not to Perform in Any Other Southeast Asian Country\"\n",
    "title_4 = \"UK University Students to Empower Cambodian Girls with STEM Skills\"\n",
    "title_5 = \"Court Detains Lamborghini Driver Behind Fatal Collision\"\n",
    "title_6 = \"AirAsia Cambodia: Enhance Intra-ASEAN Travel, Cambodian Income With Relaxed Visa Fees\"\n",
    "input_title = title_6\n",
    "recommended_articles = recommend_articles(input_title)\n",
    "print(\"Recommended Articles:\")\n",
    "recommended_articles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
